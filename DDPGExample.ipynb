{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91b58c5e",
   "metadata": {},
   "source": [
    "### DDPG on the CartPole problem\n",
    "\n",
    "We consider a variant of the CartPole problem with continuos action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8083df69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from RLTools.RLPolicies.DDPG import DDPG\n",
    "from RLTools.Utils.ReplayBuffer import ReplayBuffer\n",
    "\n",
    "# We also show how to make tensorboard logs!\n",
    "import datetime\n",
    "log_dir = 'logs/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "\n",
    "env = gym.make('InvertedPendulum-v5', reset_noise_scale=0.1)\n",
    "\n",
    "state_size = 4\n",
    "action_size = 1\n",
    "hidden_size_p = 2\n",
    "hidden_size_q = 16\n",
    "output_size = 1\n",
    "\n",
    "p_net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(state_size, hidden_size_p),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(hidden_size_p, output_size),\n",
    "    # torch.nn.Softmax(dim=-1)\n",
    ")\n",
    "q_net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(state_size+action_size, hidden_size_q),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(hidden_size_q, output_size)\n",
    "    # torch.nn.Softplus()\n",
    ")\n",
    "\n",
    "policy = DDPG(q_net, p_net, device='cuda')\n",
    "\n",
    "optimizer = torch.optim.Adam(policy.parameters(), 0.0001)\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "n_episodes = 100000\n",
    "n_rollouts = 5\n",
    "batch_size = 256\n",
    "training_iters = 5\n",
    "\n",
    "rb = ReplayBuffer() # for this algorithm we rely on a replay buffer\n",
    "\n",
    "### --- training loop\n",
    "# \n",
    "# Note that here the training is more complex as we need to implement soft updates for the V and P networks\n",
    "# for which we need to compute separate losses carefully handling parameter freezing \n",
    "\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "\n",
    "    for rollout in range(n_rollouts):\n",
    "\n",
    "\n",
    "        observation, info = env.reset()\n",
    "        episode_over = False\n",
    "\n",
    "        rewards = []\n",
    "        actions = []\n",
    "        observations = [observation]\n",
    "        log_probs = []\n",
    "        \n",
    "        while not episode_over: # unroll\n",
    "            action = policy.sample(observation, training=True)\n",
    "\n",
    "            # clip in -3,+3\n",
    "            noise = torch.normal(mean=0, std=1., size=action.shape).to(action.device)\n",
    "            action = torch.clip(action+noise, -3., 3.) \n",
    "            \n",
    "\n",
    "            actions.append(action)\n",
    "\n",
    "            observation, reward, terminated, truncated, info = env.step((action.cpu().detach().squeeze(0),))\n",
    "            rewards.append(reward)\n",
    "\n",
    "            episode_over = terminated or truncated\n",
    "            if not episode_over:\n",
    "                observations.append(observation)\n",
    "\n",
    "        writer.add_scalar(\"reward\", sum(rewards), episode*n_rollouts + rollout)\n",
    "        #rewards = torch.flip(torch.cumsum(torch.flip(torch.tensor(rewards), (0,)), 0), (0, ))\n",
    "        # print(rewards)\n",
    "        \n",
    "        rb.add(observations, actions, rewards)\n",
    "\n",
    "    for training_iter in range(training_iters):\n",
    "        batch = rb.sample(batch_size)\n",
    "\n",
    "        bstate = torch.tensor([b[0] for b in batch], dtype=torch.float32).to(policy.device)\n",
    "        baction = torch.tensor([b[1] for b in batch], dtype=torch.float32).to(policy.device).unsqueeze(1)\n",
    "        bnew_state = torch.tensor([b[2] for b in batch], dtype=torch.float32).to(policy.device)\n",
    "        breward = torch.tensor([b[3] for b in batch], dtype=torch.float32).to(policy.device)\n",
    "        bis_terminal = torch.tensor([b[4] for b in batch], dtype=torch.float32).to(policy.device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        policy.freeze_p()\n",
    "        q_loss = policy.q_loss(bstate, baction, bnew_state, breward, bis_terminal).sum()/batch_size\n",
    "        q_loss.backward()\n",
    "        policy.unfreeze_p()\n",
    "        \n",
    "        policy.freeze_q()\n",
    "        p_loss = policy.p_loss(bstate, baction, bnew_state, breward, bis_terminal).sum()/batch_size\n",
    "        p_loss.backward()\n",
    "        policy.unfreeze_q()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=0.5) # grad clipping\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        writer.add_scalar(\"Ploss\", -p_loss.item(), episode*training_iters + training_iter)\n",
    "        writer.add_scalar(\"Qloss\", q_loss.item(), episode*training_iters + training_iter)\n",
    "\n",
    "        policy.soft_update()\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8070c3f7",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f1c843e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclip(action, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3.\u001b[39m, \u001b[38;5;241m3.\u001b[39m)\n\u001b[1;32m     18\u001b[0m actions\u001b[38;5;241m.\u001b[39mappend(action)\n\u001b[0;32m---> 20\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# print(counter)\u001b[39;00m\n",
      "File \u001b[0;32m/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/wrappers/common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/envs/mujoco/inverted_pendulum_v5.py:172\u001b[0m, in \u001b[0;36mInvertedPendulumEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    169\u001b[0m info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward_survive\u001b[39m\u001b[38;5;124m\"\u001b[39m: reward}\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 172\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m observation, reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, info\n",
      "File \u001b[0;32m/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_env.py:164\u001b[0m, in \u001b[0;36mMujocoEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    Render a frame from the MuJoCo simulation as specified by the render_mode.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmujoco_renderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py:711\u001b[0m, in \u001b[0;36mMujocoRenderer.render\u001b[0;34m(self, render_mode)\u001b[0m\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m viewer\u001b[38;5;241m.\u001b[39mrender(render_mode\u001b[38;5;241m=\u001b[39mrender_mode, camera_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_id)\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m render_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 711\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py:437\u001b[0m, in \u001b[0;36mWindowViewer.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 437\u001b[0m         \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop_count \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# clear overlay\u001b[39;00m\n",
      "File \u001b[0;32m/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py:418\u001b[0m, in \u001b[0;36mWindowViewer.render.<locals>.update\u001b[0;34m()\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m gridpos, [t1, t2] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_overlays\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    409\u001b[0m         mujoco\u001b[38;5;241m.\u001b[39mmjr_overlay(\n\u001b[1;32m    410\u001b[0m             mujoco\u001b[38;5;241m.\u001b[39mmjtFontScale\u001b[38;5;241m.\u001b[39mmjFONTSCALE_150,\n\u001b[1;32m    411\u001b[0m             gridpos,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcon,\n\u001b[1;32m    416\u001b[0m         )\n\u001b[0;32m--> 418\u001b[0m \u001b[43mglfw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mswap_buffers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    419\u001b[0m glfw\u001b[38;5;241m.\u001b[39mpoll_events()\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time_per_render \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time_per_render \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m (\n\u001b[1;32m    421\u001b[0m     time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m render_start\n\u001b[1;32m    422\u001b[0m )\n",
      "File \u001b[0;32m/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/glfw/__init__.py:2275\u001b[0m, in \u001b[0;36mswap_buffers\u001b[0;34m(window)\u001b[0m\n\u001b[1;32m   2268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mswap_buffers\u001b[39m(window):\n\u001b[1;32m   2269\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;124;03m    Swaps the front and back buffers of the specified window.\u001b[39;00m\n\u001b[1;32m   2271\u001b[0m \n\u001b[1;32m   2272\u001b[0m \u001b[38;5;124;03m    Wrapper for:\u001b[39;00m\n\u001b[1;32m   2273\u001b[0m \u001b[38;5;124;03m        void glfwSwapBuffers(GLFWwindow* window);\u001b[39;00m\n\u001b[1;32m   2274\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2275\u001b[0m     \u001b[43m_glfw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglfwSwapBuffers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/glfw/__init__.py:623\u001b[0m, in \u001b[0;36m_prepare_errcheck.<locals>.errcheck\u001b[0;34m(result, *args)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_prepare_errcheck\u001b[39m():\n\u001b[1;32m    616\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;124;03m    This function sets the errcheck attribute of all ctypes wrapped functions\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;124;03m    to evaluate the _exc_info_from_callback global variable and re-raise any\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;124;03m    using the _callback_exception_decorator.\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 623\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21merrcheck\u001b[39m(result, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    624\u001b[0m         \u001b[38;5;28;01mglobal\u001b[39;00m _exc_info_from_callback\n\u001b[1;32m    625\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _exc_info_from_callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make('InvertedPendulum-v5', render_mode=\"human\", reset_noise_scale=0.3)\n",
    "\n",
    "for _ in range(10):\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    episode_over = False\n",
    "\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    observations = [observation]\n",
    "\n",
    "    counter = 0\n",
    "    while not episode_over:\n",
    "\n",
    "        #action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "        action = policy.sample(observation)\n",
    "        action = torch.clip(action, -3., 3.)\n",
    "        actions.append(action)\n",
    "        \n",
    "        observation, reward, terminated, truncated, info = env.step((action[0].cpu(),))\n",
    "        rewards.append(reward)\n",
    "        # print(counter)\n",
    "        counter +=1\n",
    "        episode_over = counter > 200 #terminated or truncated#counter > 100 #truncated #terminated # or truncated\n",
    "        if not episode_over:\n",
    "            observations.append(observation)\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
