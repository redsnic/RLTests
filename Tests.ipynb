{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2021 Horizon Robotics and ALF Contributors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import alf\n",
    "from alf.algorithms.actor_critic_algorithm import ActorCriticAlgorithm\n",
    "from alf.algorithms.trac_algorithm import TracAlgorithm\n",
    "from alf.algorithms.data_transformer import RewardScaling\n",
    "\n",
    "# environment config\n",
    "alf.config(\n",
    "    'create_environment', env_name=\"CartPole-v0\", num_parallel_environments=8)\n",
    "\n",
    "# reward scaling\n",
    "alf.config('TrainerConfig', data_transformer_ctor=RewardScaling)\n",
    "alf.config('RewardScaling', scale=0.01)\n",
    "\n",
    "# algorithm config\n",
    "alf.config('ActorDistributionNetwork', fc_layer_params=(100, ))\n",
    "alf.config('ValueNetwork', fc_layer_params=(100, ))\n",
    "alf.config(\n",
    "    'ActorCriticAlgorithm',\n",
    "    optimizer=alf.optimizers.Adam(lr=1e-3, gradient_clipping=10.0))\n",
    "alf.config(\n",
    "    'ActorCriticLoss',\n",
    "    entropy_regularization=1e-4,\n",
    "    gamma=0.98,\n",
    "    use_gae=True,\n",
    "    use_td_lambda_return=True)\n",
    "\n",
    "# training config\n",
    "alf.config(\n",
    "    'TrainerConfig',\n",
    "    unroll_length=10,\n",
    "    algorithm_ctor=TracAlgorithm,\n",
    "    num_iterations=2500,\n",
    "    num_checkpoints=5,\n",
    "    evaluate=True,\n",
    "    eval_interval=500,\n",
    "    debug_summaries=False,\n",
    "    summarize_grads_and_vars=False,\n",
    "    summary_interval=5,\n",
    "    epsilon_greedy=0.1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Initialise the environment\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "\n",
    "# Reset the environment to generate the first observation\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "    # this is where you would insert your policy\n",
    "    action = env.action_space.sample()\n",
    "    print(env.action_space)\n",
    "\n",
    "    # step (transition) through the environment with the action\n",
    "    # receiving the next observation, reward and if the episode has terminated or truncated\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # If the episode has ended then we can reset to start a new episode\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
