{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+math.exp(-x))\n",
    "\n",
    "import random\n",
    "class Bernulli():\n",
    "    \"\"\" \n",
    "    Logistic regression, manual implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, size=4):\n",
    "        self.w = [random.random()-0.5 for _ in range(size)]\n",
    "\n",
    "    def p(self, state):\n",
    "        weighted_sum = sum([self.w[i]*state[i] for i in range(len(state))])\n",
    "        return sigmoid(weighted_sum)\n",
    "\n",
    "\n",
    "    def sample(self, state):\n",
    "        return self.p(state) > random.random()\n",
    "    \n",
    "\n",
    "    def log_derivative(self, index, state): # chaining log, sigmoid, linear_layer\n",
    "        p = self.p(state)\n",
    "        return [(index - p) * s_i for s_i in state]\n",
    "\n",
    "    def update(self, alpha, grad):\n",
    "        self.w = [self.w[i] + alpha*grad[i] for i in range(len(grad))]\n",
    "\n",
    "    def policy_gradient(self, actions, rewards, states):\n",
    "        grad = [0. for _ in range(len(self.w))]\n",
    "        discount = 1.\n",
    "        for i in range(len(actions)-1, -1, -1):\n",
    "            cumulative_reward = 0.\n",
    "            for j in range(len(rewards)-1, i-1, -1):\n",
    "                cumulative_reward += (discount**(j-i))*rewards[j]\n",
    "\n",
    "            derivatives = self.log_derivative(int(actions[i]), states[i])\n",
    "            grad = [grad[i] + derivatives[i]*cumulative_reward for i in range(len(self.w))]\n",
    "        return grad           \n",
    "\n",
    "\n",
    "policy = Bernulli()\n",
    "\n",
    "for _ in tqdm(range(1000)):\n",
    "    observation, info = env.reset()\n",
    "\n",
    "\n",
    "    episode_over = False\n",
    "\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    observations = [observation]\n",
    "    while not episode_over:\n",
    "\n",
    "        #action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "        action = policy.sample(observation)\n",
    "        actions.append(action)\n",
    "\n",
    "\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        episode_over = terminated or truncated\n",
    "        if not episode_over:\n",
    "            observations.append(observation)\n",
    "\n",
    "    gradients = policy.policy_gradient(actions, rewards, observations)\n",
    "    policy.update(0.001, gradients)\n",
    "\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPPol(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, net):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.net(torch.tensor(state, dtype=torch.float32))\n",
    "    \n",
    "    def sample(self, state):\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                return self(state).multinomial(num_samples=1, replacement=True).item() \n",
    "            except:\n",
    "                print(self(state))\n",
    "                raise Exception()\n",
    "\n",
    "    def sample_training(self, state):\n",
    "        probs = self(state)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action)\n",
    "    \n",
    "    def reward(self, log_probs, rewards, gamma=0.99):\n",
    "        loss = 0\n",
    "        G = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            G = rewards[t] + gamma * G\n",
    "            loss -= log_probs[t] * G  # REINFORCE\n",
    "        return loss\n",
    "            \n",
    "    def sample_best(self,state):\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                # print(self(state))\n",
    "                return torch.argmax(self(state)).item()\n",
    "            except:\n",
    "                print(self(state))\n",
    "                raise Exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym\n",
    "# torch based implementation\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 4\n",
    "output_size = 2\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "\n",
    "\n",
    "network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(input_size, hidden_size),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(hidden_size, output_size),\n",
    "    torch.nn.Softmax(dim=-1)\n",
    ")\n",
    "\n",
    "policy = MLPPol(network)\n",
    "\n",
    "optimizer = torch.optim.Adam(policy.parameters(), 0.01)\n",
    "\n",
    "for _ in tqdm(range(1000)):\n",
    "    observation, info = env.reset()\n",
    "\n",
    "\n",
    "    episode_over = False\n",
    "\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    observations = [observation]\n",
    "    log_probs = []\n",
    "    \n",
    "    counter = 0\n",
    "    while not episode_over:\n",
    "        action, log_prob = policy.sample_training(observation)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        counter += 1\n",
    "        episode_over = terminated or truncated\n",
    "        if not episode_over:\n",
    "            observations.append(observation)\n",
    "\n",
    "    loss = policy.reward(log_probs, rewards)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "\n",
    "for _ in range(10):\n",
    "    observation, info = env.reset()\n",
    "\n",
    "\n",
    "    episode_over = False\n",
    "\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    observations = [observation]\n",
    "\n",
    "    counter = 0\n",
    "    while not episode_over:\n",
    "\n",
    "        #action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "        action = policy.sample_best(observation)\n",
    "        actions.append(action)\n",
    "\n",
    "\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        # print(counter)\n",
    "        counter +=1\n",
    "        episode_over = counter > 100 #terminated or truncated#counter > 100 #truncated #terminated # or truncated\n",
    "        if not episode_over:\n",
    "            observations.append(observation)\n",
    "\n",
    "    #gradients = policy.policy_gradient(actions, rewards, observations)\n",
    "    #policy.update(0.01, gradients)\n",
    "\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " . . . . \n",
      " .X. . . \n",
      " . .X. . \n",
      " . .A.X. \n",
      " . . . .G\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class GridEnv(gym.Env):\n",
    "    def __init__(self, size=5):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        self.observation_space = gym.spaces.Discrete(size*size)\n",
    "        self.state = None\n",
    "        self.reset()\n",
    "        self.obstacles = []\n",
    "        self.max_steps = 10\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self, state=None): # observation, info\n",
    "        # self.state = (0, 0)\n",
    "        # random initial state \n",
    "        self.state = (torch.randint(self.size-1, (1,)).item(), torch.randint(self.size-1, (1,)).item())\n",
    "        if state is not None:\n",
    "            self.state = state\n",
    "        self.current_step = 0\n",
    "        return self.state, {}\n",
    "\n",
    "    def set_obstacles(self, obstacles):\n",
    "        self.obstacles = obstacles\n",
    "        for obs in obstacles:\n",
    "            if obs[0] < 0 or obs[0] >= self.size or obs[1] < 0 or obs[1] >= self.size:\n",
    "                raise ValueError(\"Obstacle coordinates out of bounds.\")\n",
    "            if obs == (self.size - 1, self.size - 1):\n",
    "                raise ValueError(\"Obstacle cannot be at the goal position.\")\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        grid = [[' ' for _ in range(self.size)] for _ in range(self.size)]\n",
    "        # if in bounds\n",
    "        if not(self.state[0] < 0 or self.state[0] >= self.size or self.state[1] < 0 or self.state[1] >= self.size):\n",
    "            grid[self.state[0]][self.state[1]] = 'A'\n",
    "        for obs in self.obstacles:\n",
    "            grid[obs[0]][obs[1]] = 'X'\n",
    "        grid[self.size - 1][self.size - 1] = 'G'\n",
    "        if mode == 'human':\n",
    "            print('\\n'.join(['.'.join(row) for row in grid]))\n",
    "            print()\n",
    "        return grid\n",
    "\n",
    "    def translate_action_to_human(self, action):\n",
    "        action_dict = {0: 'up', 1: 'down', 2: 'left', 3: 'right'}\n",
    "        return action_dict.get(action, \"Invalid action\")\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = self.state\n",
    "        if action == 0:  # up\n",
    "            x = x - 1\n",
    "        elif action == 1:  # down\n",
    "            x = x + 1\n",
    "        elif action == 2:  # left\n",
    "            y = y - 1\n",
    "        elif action == 3:  # right\n",
    "            y = y + 1\n",
    "\n",
    "        self.state = (x, y)\n",
    "\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.max_steps:\n",
    "            return self.state, -0., False, True, {}\n",
    "\n",
    "        if (x, y) == (self.size - 1, self.size - 1): # goal\n",
    "            # print(\"goal\")\n",
    "            return self.state, 1000.0, True, False, {}\n",
    "        else:\n",
    "            for obs in self.obstacles: # wall\n",
    "                if (x, y) == obs:\n",
    "                    # print(\"obstacle\")\n",
    "                    return self.state, -1.0*(self.max_steps-self.current_step), True, False, {}\n",
    "            # if our of bounds\n",
    "            if x < 0 or x >= self.size or y < 0 or y >= self.size:\n",
    "                # print(\"out of bounds\")\n",
    "                return self.state, -1.0*(self.max_steps-self.current_step), True, False, {}\n",
    "        # If the agent moves to a valid position\n",
    "        return self.state, -0.01, False, False, {}\n",
    "\n",
    "    \n",
    "\n",
    "env = GridEnv(size=5)\n",
    "env.set_obstacles([(1, 1), (2, 2), (3, 3)])\n",
    "env.render(mode='human')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MLPAactuator' object has no attribute 'sample_training'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m episode_over:\n\u001b[0;32m---> 31\u001b[0m     action, log_prob \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_training\u001b[49m(observation)\n\u001b[1;32m     32\u001b[0m     actions\u001b[38;5;241m.\u001b[39mappend(action)\n\u001b[1;32m     33\u001b[0m     log_probs\u001b[38;5;241m.\u001b[39mappend(log_prob)\n",
      "File \u001b[0;32m/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MLPAactuator' object has no attribute 'sample_training'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_size = 2\n",
    "hidden_size = 4\n",
    "output_size = 4\n",
    "\n",
    "network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(input_size, hidden_size),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(hidden_size, output_size),\n",
    "    torch.nn.Softmax(dim=-1)\n",
    ")\n",
    "\n",
    "# policy = MLPPol(network)\n",
    "optimizer = torch.optim.Adam(policy.parameters(), 0.01)\n",
    "\n",
    "\n",
    "for _ in tqdm(range(100000)):\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    episode_over = False\n",
    "\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    observations = [observation]\n",
    "    log_probs = []\n",
    "    \n",
    "    counter = 0\n",
    "    while not episode_over:\n",
    "        action, log_prob = policy.sample_training(observation)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        # print(observation, action, reward)\n",
    "\n",
    "        counter += 1\n",
    "        episode_over = terminated or truncated\n",
    "        if not episode_over:\n",
    "            observations.append(observation)\n",
    "\n",
    "    # if sum(rewards) > 0:\n",
    "    #     print(sum(rewards), len(rewards))\n",
    "\n",
    "    loss = policy.reward(log_probs, rewards)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GridEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mGridEnv\u001b[49m(size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      2\u001b[0m env\u001b[38;5;241m.\u001b[39mset_obstacles([(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m)])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GridEnv' is not defined"
     ]
    }
   ],
   "source": [
    "env = GridEnv(size=5)\n",
    "env.set_obstacles([(1, 1), (2, 2), (3, 3)])\n",
    "\n",
    "for _ in range(1):\n",
    "\n",
    "    observation, info = env.reset(state=(0, 0))\n",
    "\n",
    "\n",
    "    episode_over = False\n",
    "\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    observations = [observation]\n",
    "\n",
    "    counter = 0\n",
    "    while not episode_over:\n",
    "        env.render()\n",
    "\n",
    "        #action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "        action = policy.sample_best(observation)\n",
    "        actions.append(action)\n",
    "        print(env.translate_action_to_human(action))\n",
    "\n",
    "\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        print(sum(rewards))\n",
    "        counter +=1\n",
    "        episode_over = terminated or truncated \n",
    "        if not episode_over:\n",
    "            observations.append(observation)\n",
    "\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25386/100000 [39:52<1:57:11, 10.61it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 197\u001b[0m\n\u001b[1;32m    194\u001b[0m action \u001b[38;5;241m=\u001b[39m policy\u001b[38;5;241m.\u001b[39msample(observation, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# clip in -3,+3\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m noise \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclip(action\u001b[38;5;241m+\u001b[39mnoise, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3.\u001b[39m, \u001b[38;5;241m3.\u001b[39m) \n\u001b[1;32m    201\u001b[0m actions\u001b[38;5;241m.\u001b[39mappend(action)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from copy import deepcopy\n",
    "\n",
    "import datetime\n",
    "log_dir = 'logs/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "\n",
    "env = gym.make('InvertedPendulum-v5', reset_noise_scale=0.1)\n",
    "\n",
    "def soft_update(target, source, tau=0.001):\n",
    "    for t_param, s_param in zip(target.parameters(), source.parameters()):\n",
    "        t_param.data.copy_(tau * s_param.data + (1.0 - tau) * t_param.data)\n",
    "\n",
    "\n",
    "class MLPAactuator(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, q_net, p_net, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.q_net = q_net.to(device)\n",
    "        self.p_net = p_net.to(device)\n",
    "        self.targ_p_net = deepcopy(p_net)\n",
    "        self.targ_q_net = deepcopy(q_net)\n",
    "        self.device = device\n",
    "    \n",
    "    def q(self, state, action):\n",
    "        sa = torch.cat([state, action], dim=1)\n",
    "        return self.q_net(sa)\n",
    "    \n",
    "    def tq(self, state, action):\n",
    "        sa = torch.cat([state, action], dim=1)\n",
    "        return self.targ_q_net(sa)\n",
    "    \n",
    "    def p(self, state):\n",
    "        return self.p_net(state)\n",
    "    \n",
    "    def tp(self, state):\n",
    "        return self.targ_p_net(state)\n",
    "\n",
    "    def _sample(self, state):\n",
    "        return self.p_net(state)\n",
    "\n",
    "    def sample(self, state, training=False):\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "        if training:\n",
    "            return self._sample(state)\n",
    "        with torch.no_grad():\n",
    "            return self._sample(state)\n",
    "\n",
    "    def q_loss(self, state, action, new_state, reward, is_terminal, gamma=0.99): # reward is computed after taking the action\n",
    "        return (self.q(state, action) - (reward + (1-is_terminal)*gamma*self.tq(new_state, self.tp(new_state))))**2\n",
    "    \n",
    "    def p_loss(self, state, action, new_state, reward, is_terminal, gamma=0.99):\n",
    "        return -self.q(state, self.p(state))\n",
    "    \n",
    "    def freeze_q(self):\n",
    "        # freeze all parameters of a neural network\n",
    "        for param in self.q_net.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.targ_p_net.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.targ_q_net.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_q(self):\n",
    "        # unfreeze all parameters of a neural network\n",
    "        for param in self.q_net.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.targ_p_net.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.targ_q_net.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def freeze_p(self):\n",
    "        # freeze all parameters of a neural network\n",
    "        for param in self.p_net.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_p(self):\n",
    "        # unfreeze all parameters of a neural network\n",
    "        for param in self.p_net.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def soft_update(self):\n",
    "        soft_update(self.targ_p_net, self.p_net)\n",
    "        soft_update(self.targ_q_net, self.q_net)\n",
    "\n",
    "    \n",
    "\n",
    "import random\n",
    "class ReplayBuffer():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.events = []\n",
    "        self.sweeping_index = 0\n",
    "\n",
    "    def add(self, states, actions, rewards):\n",
    "            if len(self.events) < 100000:\n",
    "                for i in range(len(states)):\n",
    "                    if i == len(states)-1: # terminal\n",
    "                        self.events.append([ # state, action,rewads, \n",
    "                            states[i],\n",
    "                            actions[i],\n",
    "                            states[i],\n",
    "                            rewards[i],\n",
    "                            float((i == len(states)-1)) # reaching terminal ?\n",
    "                        ])\n",
    "                    else:\n",
    "                        # print(states[i].shape, len(actions), i)\n",
    "                        self.events.append([ # state, action,rewads, \n",
    "                            states[i],\n",
    "                            actions[i],\n",
    "                            states[i+1],\n",
    "                            rewards[i],\n",
    "                            False # reaching terminal ?\n",
    "                        ])\n",
    "            else: # replace older samples\n",
    "                for i in range(len(states)):\n",
    "                    if i == len(states)-1: # terminal\n",
    "                        self.events[self.sweeping_index] = [ # state, action,rewads, \n",
    "                            states[i],\n",
    "                            actions[i],\n",
    "                            states[i],\n",
    "                            rewards[i],\n",
    "                            float((i == len(states)-1)) # reaching terminal ?\n",
    "                        ]\n",
    "                    else:\n",
    "                        # print(states[i].shape, len(actions), i)\n",
    "                        self.events[self.sweeping_index] = [ # state, action,rewads, \n",
    "                            states[i],\n",
    "                            actions[i],\n",
    "                            states[i+1],\n",
    "                            rewards[i],\n",
    "                            False # reaching terminal ?\n",
    "                        ]\n",
    "                self.sweeping_index += 1\n",
    "                if self.sweeping_index == len(self.events):\n",
    "                    self.sweeping_index = 0\n",
    "\n",
    "    def sample(self, n):\n",
    "        return random.choices(self.events, k=n) \n",
    "\n",
    "\n",
    "state_size = 4\n",
    "action_size = 1\n",
    "hidden_size_p = 2\n",
    "hidden_size_q = 16\n",
    "output_size = 1\n",
    "\n",
    "p_net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(state_size, hidden_size_p),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(hidden_size_p, output_size),\n",
    "    # torch.nn.Softmax(dim=-1)\n",
    ")\n",
    "q_net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(state_size+action_size, hidden_size_q),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(hidden_size_q, output_size)\n",
    "    # torch.nn.Softplus()\n",
    ")\n",
    "\n",
    "policy = MLPAactuator(q_net, p_net, device='cuda')\n",
    "\n",
    "optimizer = torch.optim.Adam(policy.parameters(), 0.0001)\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "n_episodes = 100000\n",
    "n_rollouts = 5\n",
    "batch_size = 256\n",
    "training_iters = 5\n",
    "\n",
    "rb = ReplayBuffer()  \n",
    "\n",
    "\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "\n",
    "    for rollout in range(n_rollouts):\n",
    "\n",
    "\n",
    "        observation, info = env.reset()\n",
    "        episode_over = False\n",
    "\n",
    "        rewards = []\n",
    "        actions = []\n",
    "        observations = [observation]\n",
    "        log_probs = []\n",
    "        \n",
    "        while not episode_over: # unroll\n",
    "            action = policy.sample(observation, training=True)\n",
    "\n",
    "            # clip in -3,+3\n",
    "            noise = torch.normal(mean=0, std=1., size=action.shape).to(action.device)\n",
    "            action = torch.clip(action+noise, -3., 3.) \n",
    "            \n",
    "\n",
    "            actions.append(action)\n",
    "\n",
    "            observation, reward, terminated, truncated, info = env.step((action.cpu().detach().squeeze(0),))\n",
    "            rewards.append(reward)\n",
    "\n",
    "            episode_over = terminated or truncated\n",
    "            if not episode_over:\n",
    "                observations.append(observation)\n",
    "\n",
    "        writer.add_scalar(\"reward\", sum(rewards), episode*n_rollouts + rollout)\n",
    "        #rewards = torch.flip(torch.cumsum(torch.flip(torch.tensor(rewards), (0,)), 0), (0, ))\n",
    "        # print(rewards)\n",
    "        \n",
    "        rb.add(observations, actions, rewards)\n",
    "\n",
    "    for training_iter in range(training_iters):\n",
    "        batch = rb.sample(batch_size)\n",
    "\n",
    "        bstate = torch.tensor([b[0] for b in batch], dtype=torch.float32).to(policy.device)\n",
    "        baction = torch.tensor([b[1] for b in batch], dtype=torch.float32).to(policy.device).unsqueeze(1)\n",
    "        bnew_state = torch.tensor([b[2] for b in batch], dtype=torch.float32).to(policy.device)\n",
    "        breward = torch.tensor([b[3] for b in batch], dtype=torch.float32).to(policy.device)\n",
    "        bis_terminal = torch.tensor([b[4] for b in batch], dtype=torch.float32).to(policy.device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        policy.freeze_p()\n",
    "        q_loss = policy.q_loss(bstate, baction, bnew_state, breward, bis_terminal).sum()/batch_size\n",
    "        q_loss.backward()\n",
    "        policy.unfreeze_p()\n",
    "        \n",
    "        policy.freeze_q()\n",
    "        p_loss = policy.p_loss(bstate, baction, bnew_state, breward, bis_terminal).sum()/batch_size\n",
    "        p_loss.backward()\n",
    "        policy.unfreeze_q()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=0.5) # grad clipping\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        writer.add_scalar(\"Ploss\", -p_loss.item(), episode*training_iters + training_iter)\n",
    "        writer.add_scalar(\"Qloss\", q_loss.item(), episode*training_iters + training_iter)\n",
    "\n",
    "        policy.soft_update()\n",
    "\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function WindowViewer.__del__ at 0x7fb2f3715c60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py\", line 359, in __del__\n",
      "    self.free()\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py\", line 352, in free\n",
      "    if glfw.get_current_context() == self.window:\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/glfw/__init__.py\", line 2264, in get_current_context\n",
      "    return _glfw.glfwGetCurrentContext()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/glfw/__init__.py\", line 628, in errcheck\n",
      "    _reraise(exc[1], exc[2])\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/glfw/__init__.py\", line 52, in _reraise\n",
      "    raise exception.with_traceback(traceback)\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/glfw/__init__.py\", line 607, in callback_wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/glfw/__init__.py\", line 820, in _handle_glfw_errors\n",
      "    raise GLFWError(message)\n",
      "glfw.GLFWError: (65537) b'The GLFW library is not initialized'\n"
     ]
    },
    {
     "ename": "GLFWError",
     "evalue": "(65537) b'The GLFW library is not initialized'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGLFWError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclip(action, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3.\u001b[39m, \u001b[38;5;241m3.\u001b[39m)\n\u001b[1;32m     19\u001b[0m actions\u001b[38;5;241m.\u001b[39mappend(action)\n\u001b[0;32m---> 21\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# print(counter)\u001b[39;00m\n",
      "File \u001b[0;32m/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/wrappers/common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/envs/mujoco/inverted_pendulum_v5.py:172\u001b[0m, in \u001b[0;36mInvertedPendulumEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    169\u001b[0m info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward_survive\u001b[39m\u001b[38;5;124m\"\u001b[39m: reward}\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 172\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m observation, reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, info\n",
      "File \u001b[0;32m/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_env.py:164\u001b[0m, in \u001b[0;36mMujocoEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    Render a frame from the MuJoCo simulation as specified by the render_mode.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmujoco_renderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py:711\u001b[0m, in \u001b[0;36mMujocoRenderer.render\u001b[0;34m(self, render_mode)\u001b[0m\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m viewer\u001b[38;5;241m.\u001b[39mrender(render_mode\u001b[38;5;241m=\u001b[39mrender_mode, camera_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_id)\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m render_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 711\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py:437\u001b[0m, in \u001b[0;36mWindowViewer.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 437\u001b[0m         \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop_count \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# clear overlay\u001b[39;00m\n",
      "File \u001b[0;32m/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py:385\u001b[0m, in \u001b[0;36mWindowViewer.render.<locals>.update\u001b[0;34m()\u001b[0m\n\u001b[1;32m    383\u001b[0m     glfw\u001b[38;5;241m.\u001b[39mdestroy_window(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow)\n\u001b[1;32m    384\u001b[0m     glfw\u001b[38;5;241m.\u001b[39mterminate()\n\u001b[0;32m--> 385\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewport\u001b[38;5;241m.\u001b[39mwidth, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewport\u001b[38;5;241m.\u001b[39mheight \u001b[38;5;241m=\u001b[39m \u001b[43mglfw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_framebuffer_size\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# update scene\u001b[39;00m\n\u001b[1;32m    389\u001b[0m mujoco\u001b[38;5;241m.\u001b[39mmjv_updateScene(\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscn,\n\u001b[1;32m    397\u001b[0m )\n",
      "File \u001b[0;32m/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/glfw/__init__.py:1301\u001b[0m, in \u001b[0;36mget_framebuffer_size\u001b[0;34m(window)\u001b[0m\n\u001b[1;32m   1299\u001b[0m height_value \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1300\u001b[0m height \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mpointer(height_value)\n\u001b[0;32m-> 1301\u001b[0m \u001b[43m_glfw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglfwGetFramebufferSize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m width_value\u001b[38;5;241m.\u001b[39mvalue, height_value\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/glfw/__init__.py:628\u001b[0m, in \u001b[0;36m_prepare_errcheck.<locals>.errcheck\u001b[0;34m(result, *args)\u001b[0m\n\u001b[1;32m    626\u001b[0m     exc \u001b[38;5;241m=\u001b[39m _exc_info_from_callback\n\u001b[1;32m    627\u001b[0m     _exc_info_from_callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m     \u001b[43m_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/glfw/__init__.py:52\u001b[0m, in \u001b[0;36m_reraise\u001b[0;34m(exception, traceback)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_reraise\u001b[39m(exception, traceback):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception\u001b[38;5;241m.\u001b[39mwith_traceback(traceback)\n",
      "File \u001b[0;32m/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/glfw/__init__.py:607\u001b[0m, in \u001b[0;36m_callback_exception_decorator.<locals>.callback_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mSystemExit\u001b[39;00m):\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/glfw/__init__.py:820\u001b[0m, in \u001b[0;36m_handle_glfw_errors\u001b[0;34m(error_code, description)\u001b[0m\n\u001b[1;32m    818\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (error_code, description)\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ERROR_REPORTING \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexception\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 820\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GLFWError(message)\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ERROR_REPORTING \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarning\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    822\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, GLFWError)\n",
      "\u001b[0;31mGLFWError\u001b[0m: (65537) b'The GLFW library is not initialized'"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make('InvertedPendulum-v5', render_mode=\"human\", reset_noise_scale=0.01)\n",
    "\n",
    "for _ in range(10):\n",
    "    observation, info = env.reset()\n",
    "\n",
    "\n",
    "    episode_over = False\n",
    "\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    observations = [observation]\n",
    "\n",
    "    counter = 0\n",
    "    while not episode_over:\n",
    "\n",
    "        #action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "        action = policy.sample(observation)\n",
    "        action = torch.clip(action, -3., 3.)\n",
    "        actions.append(action)\n",
    "        \n",
    "        observation, reward, terminated, truncated, info = env.step((action[0].cpu(),))\n",
    "        rewards.append(reward)\n",
    "        # print(counter)\n",
    "        counter +=1\n",
    "        episode_over = counter > 100 #terminated or truncated#counter > 100 #truncated #terminated # or truncated\n",
    "        if not episode_over:\n",
    "            observations.append(observation)\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
