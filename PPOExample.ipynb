{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecc8dbe2",
   "metadata": {},
   "source": [
    "### CartPole with PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac180e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym\n",
    "from RLTools.RLPolicies.PPO import PPO\n",
    "# torch based implementation\n",
    "\n",
    "state_size = 4\n",
    "action_size = 1\n",
    "hidden_size = 4\n",
    "output_size = 2\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "p_net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(state_size, hidden_size),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(hidden_size, output_size),\n",
    "    torch.nn.Softmax(dim=-1)\n",
    ")\n",
    "\n",
    "v_net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(state_size, hidden_size),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(hidden_size, 1)\n",
    ")\n",
    "\n",
    "policy = PPO(p_net, v_net)\n",
    "\n",
    "optimizer = torch.optim.Adam(policy.parameters(), 0.001)\n",
    "\n",
    "update_policy_every = 5\n",
    "\n",
    "counter = 0\n",
    "for _ in tqdm(range(10000)):\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    episode_over = False\n",
    "\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    observations = [observation]\n",
    "    log_probs = []\n",
    "    entropies = []\n",
    "\n",
    "    while not episode_over:\n",
    "        action, log_prob, entropy = policy.sample_training(observation)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        entropies.append(entropy)\n",
    "\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        episode_over = terminated or truncated\n",
    "        if not episode_over:\n",
    "            observations.append(observation)\n",
    "\n",
    "    loss = policy.reward(observations, actions, rewards, entropies).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if counter % update_policy_every == 0 and counter > 0:\n",
    "        policy.swap()\n",
    "    counter += 1\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b18c27c",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e74240",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "\n",
    "for _ in range(10):\n",
    "    observation, info = env.reset()\n",
    "\n",
    "\n",
    "    episode_over = False\n",
    "\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    observations = [observation]\n",
    "\n",
    "    counter = 0\n",
    "    while not episode_over:\n",
    "\n",
    "        #action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "        action = policy.sample_best(observation)\n",
    "        actions.append(action)\n",
    "\n",
    "\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        # print(counter)\n",
    "        counter +=1\n",
    "        episode_over = counter > 100 #terminated or truncated#counter > 100 #truncated #terminated # or truncated\n",
    "        if not episode_over:\n",
    "            observations.append(observation)\n",
    "\n",
    "    #gradients = policy.policy_gradient(actions, rewards, observations)\n",
    "    #policy.update(0.01, gradients)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16229c53",
   "metadata": {},
   "source": [
    "### Now let's try lunar lander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b89a0600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200000 [00:00<?, ?it/s]/local0/scratch/git/RLTests/RLTools/RLPolicies/PPO.py:47: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
      "  3%|â–Ž         | 5756/200000 [43:43<24:35:38,  2.19it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m entropies \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m episode_over:\n\u001b[0;32m---> 55\u001b[0m     action, log_prob, entropy \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     actions\u001b[38;5;241m.\u001b[39mappend(action)\n\u001b[1;32m     57\u001b[0m     log_probs\u001b[38;5;241m.\u001b[39mappend(log_prob)\n",
      "File \u001b[0;32m/local0/scratch/git/RLTests/RLTools/RLPolicies/PPO.py:78\u001b[0m, in \u001b[0;36mPPO.sample_training\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     76\u001b[0m action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m     77\u001b[0m entropy \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mentropy()\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\u001b[38;5;241m.\u001b[39mitem(), \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m, entropy\n",
      "File \u001b[0;32m/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/torch/distributions/categorical.py:138\u001b[0m, in \u001b[0;36mCategorical.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_args:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_sample(value)\n\u001b[0;32m--> 138\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    139\u001b[0m value, log_pmf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits)\n\u001b[1;32m    140\u001b[0m value \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym\n",
    "from RLTools.RLPolicies.PPO import PPO\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# torch based implementation\n",
    "\n",
    "state_size = 8\n",
    "action_size = 1\n",
    "hidden_size = 16\n",
    "output_size = 4\n",
    "\n",
    "env = gym.make('LunarLander-v3')\n",
    "\n",
    "p_net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(state_size, hidden_size),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(hidden_size, hidden_size),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(hidden_size, output_size),\n",
    "    torch.nn.Softmax(dim=-1)\n",
    ")\n",
    "\n",
    "v_net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(state_size, hidden_size),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(hidden_size, hidden_size),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(hidden_size, 1)\n",
    ")\n",
    "\n",
    "policy = PPO(p_net, v_net, device='cuda')\n",
    "\n",
    "optimizer = torch.optim.Adam(policy.parameters(), 0.01)\n",
    "\n",
    "update_policy_every = 5\n",
    "\n",
    "import datetime\n",
    "log_dir = 'logs/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "counter = 0\n",
    "for episode in tqdm(range(200000)):\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    episode_over = False\n",
    "\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    observations = [observation]\n",
    "    log_probs = []\n",
    "    entropies = []\n",
    "\n",
    "    while not episode_over:\n",
    "        action, log_prob, entropy = policy.sample_training(observation)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        entropies.append(entropy)\n",
    "\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        episode_over = terminated or truncated\n",
    "        if not episode_over:\n",
    "            observations.append(observation)\n",
    "\n",
    "    loss = policy.reward(observations, actions, rewards, entropies).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    writer.add_scalar(\"loss\", loss.item(), episode)\n",
    "    writer.add_scalar(\"reward\", sum(rewards), episode)\n",
    "\n",
    "    if counter % update_policy_every == 0 and counter > 0:\n",
    "        policy.swap()\n",
    "    counter += 1\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e969f080",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v3', render_mode=\"human\")\n",
    "\n",
    "for _ in range(10):\n",
    "    observation, info = env.reset()\n",
    "\n",
    "\n",
    "    episode_over = False\n",
    "\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    observations = [observation]\n",
    "\n",
    "    counter = 0\n",
    "    while not episode_over:\n",
    "\n",
    "        #action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "        action = policy.sample_best(observation)\n",
    "        actions.append(action)\n",
    "\n",
    "\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        # print(counter)\n",
    "        counter +=1\n",
    "        episode_over = terminated or truncated#counter > 100 #truncated #terminated # or truncated\n",
    "        if not episode_over:\n",
    "            observations.append(observation)\n",
    "\n",
    "    #gradients = policy.policy_gradient(actions, rewards, observations)\n",
    "    #policy.update(0.01, gradients)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c369fe89",
   "metadata": {},
   "source": [
    "### Vectorized Lunar Lander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f2ae5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 26/200000 [00:54<92:39:11,  1.67s/it] /local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:573: UserWarning: \u001b[33mWARN: Calling `close` while waiting for a pending call to `step` to complete.\u001b[0m\n",
      "  logger.warn(\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-10 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Traceback (most recent call last):\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py\", line 701, in _async_worker\n",
      "    command, data = pipe.recv()\n",
      "                    ^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-35 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Traceback (most recent call last):\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py\", line 723, in _async_worker\n",
      "    ) = env.step(data)\n",
      "        ^^^^^^^^^^^^^^\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/wrappers/common.py\", line 125, in step\n",
      "    observation, reward, terminated, truncated, info = self.env.step(action)\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/wrappers/common.py\", line 393, in step\n",
      "    return super().step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/core.py\", line 322, in step\n",
      "    return self.env.step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/wrappers/common.py\", line 285, in step\n",
      "    return self.env.step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/envs/box2d/lunar_lander.py\", line 527, in step\n",
      "    dispersion = [self.np_random.uniform(-1.0, +1.0) / SCALE for _ in range(2)]\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/envs/box2d/lunar_lander.py\", line 527, in <listcomp>\n",
      "    dispersion = [self.np_random.uniform(-1.0, +1.0) / SCALE for _ in range(2)]\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-32 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-16 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-1 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-27 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-29 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-13 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-63 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Traceback (most recent call last):\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py\", line 723, in _async_worker\n",
      "    ) = env.step(data)\n",
      "        ^^^^^^^^^^^^^^\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/wrappers/common.py\", line 125, in step\n",
      "    observation, reward, terminated, truncated, info = self.env.step(action)\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/wrappers/common.py\", line 393, in step\n",
      "    return super().step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/core.py\", line 322, in step\n",
      "    return self.env.step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/wrappers/common.py\", line 285, in step\n",
      "    return self.env.step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/envs/box2d/lunar_lander.py\", line 514, in step\n",
      "    assert self.action_space.contains(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/spaces/discrete.py\", line 106, in contains\n",
      "    return bool(self.start <= as_int64 < self.start + self.n)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-47 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-9 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-5 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-22 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-57 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-50 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-26 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-25 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-12 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-42 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-43 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-18 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-8 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-54 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-30 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Traceback (most recent call last):\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py\", line 714, in _async_worker\n",
      "    observation, info = env.reset()\n",
      "                        ^^^^^^^^^^^\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/wrappers/common.py\", line 146, in reset\n",
      "    return super().reset(seed=seed, options=options)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/core.py\", line 328, in reset\n",
      "    return self.env.reset(seed=seed, options=options)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/wrappers/common.py\", line 400, in reset\n",
      "    return super().reset(seed=seed, options=options)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/core.py\", line 328, in reset\n",
      "    return self.env.reset(seed=seed, options=options)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/wrappers/common.py\", line 295, in reset\n",
      "    return self.env.reset(seed=seed, options=options)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/envs/box2d/lunar_lander.py\", line 358, in reset\n",
      "    for i in range(CHUNKS)\n",
      "             ^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-19 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-37 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-41 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-4 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-34 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Received the following error from Worker-55 - Shutting it down\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:422: UserWarning: \u001b[31mERROR: Raising the last exception back to the main process.\u001b[0m\n",
      "  self._raise_if_errors(successes)\n",
      "Exception ignored in: <function AsyncVectorEnv.__del__ at 0x7f111d845b20>\n",
      "Traceback (most recent call last):\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py\", line 681, in __del__\n",
      "    self.close(terminate=True)\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/vector_env.py\", line 222, in close\n",
      "    self.close_extras(**kwargs)\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py\", line 577, in close_extras\n",
      "    function(timeout)\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py\", line 422, in step_wait\n",
      "    self._raise_if_errors(successes)\n",
      "  File \"/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py\", line 676, in _raise_if_errors\n",
      "    raise exctype(value)\n",
      "KeyboardInterrupt: \n",
      "  0%|          | 478/200000 [1:10:06<487:44:27,  8.80s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 63\u001b[0m\n\u001b[1;32m     61\u001b[0m done_envs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(n_env, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m episode_over:\n\u001b[0;32m---> 63\u001b[0m     action, log_prob, entropy \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     actions\u001b[38;5;241m.\u001b[39mappend(action)\n\u001b[1;32m     65\u001b[0m     log_probs\u001b[38;5;241m.\u001b[39mappend(log_prob)\n",
      "File \u001b[0;32m/local0/scratch/git/RLTests/RLTools/RLPolicies/PPO.py:191\u001b[0m, in \u001b[0;36mBPPO.sample_training\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    189\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet(torch\u001b[38;5;241m.\u001b[39mtensor(state)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[1;32m    190\u001b[0m dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(probs)\n\u001b[0;32m--> 191\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m entropy \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mentropy()\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action, dist\u001b[38;5;241m.\u001b[39mlog_prob(action), entropy\n",
      "File \u001b[0;32m/local0/scratch/git/RLTests/.venv/lib/python3.11/site-packages/torch/distributions/categorical.py:132\u001b[0m, in \u001b[0;36mCategorical.sample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m    130\u001b[0m     sample_shape \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize(sample_shape)\n\u001b[1;32m    131\u001b[0m probs_2d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events)\n\u001b[0;32m--> 132\u001b[0m samples_2d \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples_2d\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extended_shape(sample_shape))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym\n",
    "from RLTools.RLPolicies.PPO import PPO, BPPO\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "# torch based implementation\n",
    "\n",
    "state_size = 8\n",
    "action_size = 1\n",
    "hidden_size = 16\n",
    "output_size = 4\n",
    "\n",
    "# env = gym.make('LunarLander-v3')\n",
    "\n",
    "n_env = 64\n",
    "env = gym.vector.AsyncVectorEnv([lambda: gym.make('LunarLander-v3') for _ in range(n_env)]) # reminder gym.vector.AsyncVectorEnv allows different envs!\n",
    "\n",
    "\n",
    "p_net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(state_size, hidden_size),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(hidden_size, hidden_size),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(hidden_size, output_size),\n",
    "    torch.nn.Softmax(dim=-1)\n",
    ")\n",
    "\n",
    "v_net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(state_size, hidden_size),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(hidden_size, hidden_size),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(hidden_size, 1)\n",
    ")\n",
    "\n",
    "policy = BPPO(p_net, v_net, device='cuda')\n",
    "\n",
    "optimizer = torch.optim.Adam(policy.parameters(), 0.01)\n",
    "\n",
    "update_policy_every = 5\n",
    "update_gradients_every = 1\n",
    "\n",
    "import datetime\n",
    "log_dir = 'logs/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "counter_for_swap = 0\n",
    "counter_for_backward = 0\n",
    "for episode in tqdm(range(200000)):\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    episode_over = False\n",
    "\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    observations = [observation]\n",
    "    log_probs = []\n",
    "    entropies = []\n",
    "\n",
    "    done_envs = np.zeros(n_env, dtype=bool)\n",
    "    while not episode_over:\n",
    "        action, log_prob, entropy = policy.sample_training(observation)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        entropies.append(entropy)\n",
    "\n",
    "\n",
    "\n",
    "        observation, reward, terminated, truncated, info = env.step(action.cpu().numpy())\n",
    "        rewards.append(reward)\n",
    "\n",
    "        done_envs |= (terminated | truncated)\n",
    "        episode_over = np.all(done_envs)\n",
    "        if not episode_over:\n",
    "            observations.append(observation)\n",
    "\n",
    "    # Tensorize\n",
    "    obs_seq = [torch.tensor([observations[t][i] for t in range(len(observations))], dtype=torch.float32) for i in range(n_env)]\n",
    "    act_seq = [torch.tensor([actions[t][i] for t in range(len(actions))], dtype=torch.int64) for i in range(n_env)]\n",
    "    rew_seq = [torch.tensor([rewards[t][i] for t in range(len(rewards))], dtype=torch.float32) for i in range(n_env)]\n",
    "    logp_seq = [torch.stack([log_probs[t][i] for t in range(len(log_probs))]) for i in range(n_env)]\n",
    "    entr_seq = [torch.stack([entropies[t][i] for t in range(len(entropies))]) for i in range(n_env)]\n",
    "    # Pad\n",
    "    obs_tensor = torch.nn.utils.rnn.pad_sequence(obs_seq, batch_first=True).to(policy.device)         # [batch_size, T, state_size]\n",
    "    actions_tensor = torch.nn.utils.rnn.pad_sequence(act_seq, batch_first=True).to(policy.device)     # [batch_size, T]\n",
    "    rewards_tensor = torch.nn.utils.rnn.pad_sequence(rew_seq, batch_first=True).to(policy.device)     # [batch_size, T]\n",
    "    log_probs_tensor = torch.nn.utils.rnn.pad_sequence(logp_seq, batch_first=True).to(policy.device)  # [batch_size, T]\n",
    "    entropies_tensor = torch.nn.utils.rnn.pad_sequence(entr_seq, batch_first=True).to(policy.device)  # [batch_size, T]\n",
    "    # mask\n",
    "    lengths = torch.tensor([len(seq) for seq in rew_seq], device=policy.device)    # [batch_size]\n",
    "    max_len = rewards_tensor.size(1)  # time dimension\n",
    "    mask = torch.arange(max_len, device=policy.device).unsqueeze(0) < lengths.unsqueeze(1)  # [batch_size, T]\n",
    "\n",
    "    loss = policy.reward(obs_tensor, actions_tensor, rewards_tensor, entropies_tensor, mask).mean()\n",
    "    if counter_for_backward % update_gradients_every == 0 and counter_for_backward > 0:\n",
    "        loss.backward()\n",
    "        counter_for_swap += 1 \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        writer.add_scalar(\"loss\", loss.item(), episode)\n",
    "        writer.add_scalar(\"reward\", rewards_tensor.sum(dim=1).mean(), episode)\n",
    "\n",
    "    if counter_for_swap % update_policy_every == 0 and counter_for_swap > 0:\n",
    "        policy.swap()\n",
    "\n",
    "    counter_for_backward += 1\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8b1a1f",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cea0689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v3', render_mode=\"human\")\n",
    "\n",
    "for _ in range(10):\n",
    "    observation, info = env.reset()\n",
    "\n",
    "\n",
    "    episode_over = False\n",
    "\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    observations = [observation]\n",
    "\n",
    "    counter = 0\n",
    "    while not episode_over:\n",
    "\n",
    "        #action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "        action = policy.sample_best(observation)\n",
    "        actions.append(action)\n",
    "\n",
    "\n",
    "        observation, reward, terminated, truncated, info = env.step(action.cpu().numpy())\n",
    "        rewards.append(reward)\n",
    "        # print(counter)\n",
    "        counter +=1\n",
    "        episode_over = terminated or truncated#counter > 100 #truncated #terminated # or truncated\n",
    "        if not episode_over:\n",
    "            observations.append(observation)\n",
    "\n",
    "    #gradients = policy.policy_gradient(actions, rewards, observations)\n",
    "    #policy.update(0.01, gradients)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
