{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ec29f97",
   "metadata": {},
   "source": [
    "### Solving the CartPole problem with Logistic regression\n",
    "\n",
    "Here we show an implementation of the REINFORCE algorithm to learn the parameters for solving the CartPole problem on a finite time horizon. \n",
    "This considers that we need to learn the parmeter p of a Bernoulli distribution where the p is parametrized by a $\\sigma(Xw)$, with w being the state\n",
    "of the CartPole system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18919170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:16<00:00, 61.92it/s]\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+math.exp(-x))\n",
    "\n",
    "import random\n",
    "class Bernulli():\n",
    "    \"\"\" \n",
    "    Logistic regression, manual implementation. \n",
    "    \"\"\"\n",
    "    def __init__(self, size=4): # random weights \n",
    "        self.w = [random.random()-0.5 for _ in range(size)]\n",
    "\n",
    "    def p(self, state): # forward pass\n",
    "        weighted_sum = sum([self.w[i]*state[i] for i in range(len(state))])\n",
    "        return sigmoid(weighted_sum)\n",
    "\n",
    "    def sample(self, state): # randomly sample action given current p\n",
    "        return self.p(state) > random.random()\n",
    "    \n",
    "    def sample_best(self, state): # randomly sample action given current p\n",
    "        return self.p(state) > 0.5\n",
    "    \n",
    "    def log_derivative(self, index, state): # chaining log, sigmoid, linear_layer\n",
    "        p = self.p(state)\n",
    "        return [(index - p) * s_i for s_i in state]\n",
    "\n",
    "    def update(self, alpha, grad): # gradient update\n",
    "        self.w = [self.w[i] + alpha*grad[i] for i in range(len(grad))]\n",
    "\n",
    "    def policy_gradient(self, actions, rewards, states): # gradient computation\n",
    "        grad = [0. for _ in range(len(self.w))]\n",
    "        discount = 1.\n",
    "        for i in range(len(actions)-1, -1, -1):\n",
    "            cumulative_reward = 0.\n",
    "            for j in range(len(rewards)-1, i-1, -1):\n",
    "                cumulative_reward += (discount**(j-i))*rewards[j]\n",
    "\n",
    "            derivatives = self.log_derivative(int(actions[i]), states[i])\n",
    "            grad = [grad[i] + derivatives[i]*cumulative_reward for i in range(len(self.w))]\n",
    "        return grad           \n",
    "\n",
    "# Training\n",
    "\n",
    "policy = Bernulli() # create the policy\n",
    "\n",
    "\n",
    "### -- training loop\n",
    "\n",
    "for _ in tqdm(range(1000)):\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    episode_over = False\n",
    "\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    observations = [observation]\n",
    "    while not episode_over:\n",
    "\n",
    "        #action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "        action = policy.sample(observation)\n",
    "        actions.append(action)\n",
    "\n",
    "\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        episode_over = terminated or truncated\n",
    "        if not episode_over:\n",
    "            observations.append(observation)\n",
    "\n",
    "    gradients = policy.policy_gradient(actions, rewards, observations)\n",
    "    policy.update(0.001, gradients)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7715caab",
   "metadata": {},
   "source": [
    "### Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "531830db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "\n",
    "for _ in range(10):\n",
    "    observation, info = env.reset()\n",
    "\n",
    "\n",
    "    episode_over = False\n",
    "\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    observations = [observation]\n",
    "\n",
    "    counter = 0\n",
    "    while not episode_over:\n",
    "\n",
    "        #action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "        action = policy.sample_best(observation)\n",
    "        actions.append(action)\n",
    "\n",
    "\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        # print(counter)\n",
    "        counter +=1\n",
    "        episode_over = counter > 100 #terminated or truncated#counter > 100 #truncated #terminated # or truncated\n",
    "        if not episode_over:\n",
    "            observations.append(observation)\n",
    "\n",
    "    #gradients = policy.policy_gradient(actions, rewards, observations)\n",
    "    #policy.update(0.01, gradients)\n",
    "\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
